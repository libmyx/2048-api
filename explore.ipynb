{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from game2048.game import Game\n",
    "from game2048.displays import Display, IPythonDisplay\n",
    "from game2048.agents import Agent, RandomAgent, ExpectiMaxAgent\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "display1 = Display()\n",
    "display2 = IPythonDisplay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(4, random=False)\n",
    "display1.display(game)\n",
    "display2.display(game)\n",
    "agent = RandomAgent(game, display=display1)\n",
    "agent.play(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(4, random=False)\n",
    "agent = Agent(game, display=display2)\n",
    "agent.play(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "game = Game(4, score_to_win=2048, random=False)\n",
    "display2.display(game)\n",
    "agent = ExpectiMaxAgent(game, display=display2)\n",
    "agent.play(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# a random start\n",
    "game = Game(4, score_to_win=2048, random=True)\n",
    "display2.display(game)\n",
    "agent = ExpectiMaxAgent(game, display=display2)\n",
    "agent.play(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(\"The ExpectiMax agent always search a fixed solution given certain board:\",\n",
    "          agent.step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running the loop manually...\")\n",
    "\n",
    "game = Game(4, random=False, enable_rewrite_board=False)\n",
    "agent = RandomAgent(game)\n",
    "\n",
    "for _ in range(10):\n",
    "    direction = agent.step()\n",
    "    print(\"Moving to direction `%s`...\"%direction)\n",
    "    game.move(direction)\n",
    "    display1.display(game)\n",
    "    display2.display(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disable to rewrite `board` manually.\n"
     ]
    }
   ],
   "source": [
    "game.board = game.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game.enable_rewrite_board = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board = game.board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:\t       4      32      64     512\n",
      "\t       2     512       2       4\n",
      "\t       8      64      64     256\n",
      "\t      32     256     256      32\n",
      "Score: 512\n",
      "State:\t       4      32      64     512\n",
      "\t       2     512       2       4\n",
      "\t       8      64      64     256\n",
      "\t      32     256     256      32\n",
      "Score: 512\n",
      "Loaded expectmax lib for 2048: /home/myx/learning_affairs/EE228/project/2048-api/game2048/expectimax/bin/2048.so\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(4, random=True, score_to_win=2048)\n",
    "randomAgent = RandomAgent(game, display = display1)\n",
    "display1.display(game)\n",
    "randomAgent.step()\n",
    "display1.display(game)\n",
    "expectimax = ExpectiMaxAgent(game)\n",
    "expectimax.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, game, display=None):\n",
    "        self.game = game\n",
    "        self.memory = deque(maxlen=500000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 1000000\n",
    "\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 20000\n",
    "        self.state_size = 16\n",
    "        self.action_size = 4\n",
    "        self.learning_rate = 0.00025\n",
    "\n",
    "        self.evaluation_model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        creates evaluation and train model\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(16,),  activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "        model.add(Dense(4, activation='softmax', kernel_initializer=\"he_uniform\"))\n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def step(self, count):\n",
    "        \"\"\"\n",
    "        returns the action chosen by this agent based on how many count of steps it has gone\n",
    "        \"\"\"\n",
    "        if count > 10:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decay\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, 4)\n",
    "        return np.argmax(self.evaluation_model.predict(self.game.board.reshape(1,16))[0])\n",
    "\n",
    "    def remember(self, observation, action, reward, observation_, done, teacher_action):\n",
    "        \"\"\"\n",
    "        stores the transition in memory\n",
    "        \"\"\"\n",
    "        self.memory.append((observation, action, reward, observation_, done, teacher_action))\n",
    "        return\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        replays memory to learn\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "\n",
    "        max_batch = random.sample(self.memory, self.batch_size * 10)\n",
    "        half_batch = sorted(max_batch, key=lambda i: i[2], reverse=True)[:self.batch_size // 2 + 1]\n",
    "        random_batch = random.sample(self.memory, self.batch_size // 2 + 1)\n",
    "        mini_batch = half_batch + random_batch\n",
    "\n",
    "        update_input = np.zeros((self.batch_size, self.state_size))\n",
    "        update_target = np.zeros((self.batch_size, self.action_size))\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state, action, reward, new_state, done, teacher_action = mini_batch[i]\n",
    "            target = self.evaluation_model.predict(state)[0]\n",
    "\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.gamma * np.amax(self.target_model.predict(new_state)[0])\n",
    "\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        self.evaluation_model.fit(update_input, update_target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "        return\n",
    "\n",
    "    def target_train(self):\n",
    "        \"\"\"\n",
    "        give the target network the weight of the network evaluation\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.evaluation_model.get_weights())\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1001\n",
    "trial_len = 1000\n",
    "tmp_reward = 0\n",
    "sum_rewards = 0\n",
    "graph_reward = []\n",
    "graph_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "48.54\n",
      "100\n",
      "14.68\n",
      "200\n",
      "14.5\n",
      "300\n",
      "14.87\n",
      "400\n",
      "14.48\n",
      "500\n",
      "14.43\n",
      "600\n",
      "15.5\n",
      "700\n",
      "14.8\n",
      "800\n",
      "14.6\n",
      "900\n",
      "14.89\n",
      "1000\n",
      "15.4\n"
     ]
    }
   ],
   "source": [
    "game = Game(4, random=False, score_to_win=2048)\n",
    "dqn_agent = DQNAgent(game)\n",
    "count = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    game = Game(4, random=False, score_to_win=2048)\n",
    "    dqn_agent.game = game\n",
    "    observation = dqn_agent.game.board.reshape(1,16)\n",
    "    tmp_reward = 0\n",
    "    reward = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        action = dqn_agent.step(count)\n",
    "        expectimax = ExpectiMaxAgent(game)\n",
    "        teacher_action = expectimax.step()\n",
    "        reward, done = dqn_agent.game.move(action)\n",
    "        observation_ = dqn_agent.game.board.reshape(1,16)\n",
    "        reward = int(teacher_action==action)\n",
    "        tmp_reward += reward\n",
    "        dqn_agent.remember(observation, action, reward, observation_, done, teacher_action)\n",
    "        if count%4 == 0:\n",
    "            dqn_agent.replay()\n",
    "        observation = observation_\n",
    "        if done:\n",
    "            if count >= 2500 and count%2500 == 0:\n",
    "                dqn_agent.target_train()\n",
    "            break\n",
    "    sum_rewards += tmp_reward\n",
    "    if episode % 100 == 0:\n",
    "        print(episode)\n",
    "        print(sum_rewards/100)\n",
    "        sum_rewards = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rl-2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game2048.game import Game\n",
    "from game2048.displays import Display, IPythonDisplay\n",
    "from game2048.agents import Agent, RandomAgent, ExpectiMaxAgent\n",
    "\n",
    "max_tile = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting observations in range (0,1) using log(n)/log(max) so that gradients don't vanish\n",
    "def process_log(observation):\n",
    "        observation_temp = np.where(observation <= 0, 1, observation) \n",
    "        processed_observation = np.log2(observation_temp)/np.log2(65536)\n",
    "        return processed_observation.reshape(1,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grids_next_step(grid):\n",
    "        #Returns the next 4 states s' from the current state s\n",
    "        \n",
    "        grids_list = []\n",
    "        \n",
    "        for movement in range(4):\n",
    "            grid_before = grid.copy()\n",
    "            env1 = Game(4, random=False, enable_rewrite_board=True)\n",
    "            env1.board = grid_before\n",
    "            try:\n",
    "                _ = env1.move(movement) \n",
    "            except:\n",
    "                pass\n",
    "            grid_after = env1.board\n",
    "            grids_list.append(grid_after)\n",
    "            \n",
    "        return grids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self,env):\n",
    "        #Defining the hyperparameters for the model\n",
    "        self.env=env.board\n",
    "        #The replay memory will be stored in a Deque\n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.90\n",
    "        #self.epsilon=1.0\n",
    "        self.epsilon_min=0.01\n",
    "        self.epsilon_decay=0.995\n",
    "        self.learning_rate=0.005\n",
    "        self.epsilon=pow(self.epsilon_decay, 1000)\n",
    "        self.tau=0.125\n",
    "        #We use 2 models to prevent Bootstrapping\n",
    "        #self.model=self.create_model()\n",
    "        #self.target_model=self.create_model()\n",
    "        self.model = keras.models.load_model('result1/trial num-4767.h5f')\n",
    "        self.target_model = keras.models.load_model('result1/trial num-4767.h5f')\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        model=Sequential()\n",
    "        state_shape=4\n",
    "                \n",
    "        model.add(Flatten(input_shape=(4,4)))\n",
    "        model.add(Dense(units=1024,activation=\"relu\"))\n",
    "        model.add(Dense(units=512,activation=\"relu\"))\n",
    "        model.add(Dense(units=256,activation=\"relu\"))\n",
    "        model.add(Dense(units=4))\n",
    "        model.compile(loss=\"mean_squared_error\",optimizer=Adam(lr=self.learning_rate))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "                  \n",
    "    def act(self,state):\n",
    "        #Epsilon value decays as model gains experience\n",
    "        self.epsilon*=self.epsilon_decay\n",
    "        self.epsilon=max(self.epsilon_min,self.epsilon)\n",
    "        if np.random.random()<self.epsilon:\n",
    "                  return np.random.randint(0,4)\n",
    "        else:\n",
    "            #Getting the 4 future states\n",
    "            allstates=get_grids_next_step(state)\n",
    "            \n",
    "            res=[]\n",
    "            for i in range(len(allstates)):\n",
    "                if (allstates[i]==state).all():\n",
    "                    res.append(0)\n",
    "                else:\n",
    "                    processed_state=process_log(allstates[i])\n",
    "                    #max from the 4 future Q_Values is appended in res\n",
    "                    res.append(np.max(self.model.predict(processed_state)))\n",
    "            \n",
    "            a=self.model.predict(process_log(state))\n",
    "            #Final Q_Values are the sum of Q_Values of current state andfuture states\n",
    "            final=np.add(a,res)\n",
    "            \n",
    "            return np.argmax(final)\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        #Replay Memory stores tuple(S, A, R, S')\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size=32\n",
    "        if len(self.memory)<batch_size:\n",
    "            return\n",
    "        samples=random.sample(self.memory,batch_size)\n",
    "        for sample in samples:\n",
    "            \n",
    "            state,action,reward,new_state,done=sample\n",
    "            \n",
    "            target=self.target_model.predict(process_log(state))\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                target[0][action]=reward\n",
    "            else:\n",
    "                #Bellman Equation for update\n",
    "                Q_future=max(self.target_model.predict(process_log(new_state))[0])\n",
    "                \n",
    "                #The move which was selected, its Q_Value gets updated\n",
    "                target[0][action]=reward+Q_future*self.gamma\n",
    "            self.model.fit((process_log(state)),target,epochs=1,verbose=0)\n",
    "                  \n",
    "                  \n",
    "    def target_train(self):\n",
    "        weights=self.model.get_weights()\n",
    "        target_weights=self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i]=weights[i]*self.tau+target_weights[i]*(1-self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "                  \n",
    "                  \n",
    "    def save_model(self,fn):\n",
    "        self.model.save(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "128\n",
      "256\n",
      "128\n",
      "64\n",
      "128\n",
      "32\n",
      "128\n",
      "64\n",
      "128\n",
      "128\n",
      "256\n",
      "128\n",
      "64\n",
      "128\n",
      "256\n",
      "128\n",
      "128\n",
      "64\n",
      "64\n",
      "128\n",
      "128\n",
      "64\n",
      "128\n",
      "128\n",
      "128\n",
      "64\n",
      "64\n",
      "64\n",
      "128\n",
      "128\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "256\n",
      "256\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "128\n",
      "64\n",
      "64\n",
      "256\n",
      "128\n",
      "64\n",
      "128\n",
      "128\n",
      "128\n",
      "64\n",
      "64\n",
      "64\n",
      "128\n",
      "128\n",
      "64\n",
      "128\n",
      "128\n",
      "128\n",
      "256\n",
      "64\n",
      "64\n",
      "64\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "64\n",
      "64\n",
      "256\n",
      "32\n",
      "256\n",
      "128\n",
      "64\n",
      "256\n",
      "128\n",
      "128\n",
      "128\n",
      "256\n",
      "128\n",
      "64\n",
      "128\n",
      "64\n",
      "64\n",
      "256\n",
      "128\n",
      "64\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "32\n",
      "128\n",
      "128\n",
      "128\n",
      "256\n",
      "64\n",
      "32\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = Game(4, random=False)\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.95\n",
    "    \n",
    "    trials = 100\n",
    "    trial_len = 5000\n",
    "    dqn_agent = DQN(env=env)\n",
    "    steps = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for trial in range(trials):\n",
    "        env = Game(4, random=False)\n",
    "        cur_state=env.board\n",
    "        stepno=0\n",
    "        \n",
    "        for step in range(trial_len):\n",
    "            stepno+=1\n",
    "            \n",
    "            action=dqn_agent.act(cur_state)\n",
    "            \n",
    "            reward,done = env.move(action)\n",
    "            new_state=env.board\n",
    "            \n",
    "            dqn_agent.remember(cur_state,action,reward,new_state,done)\n",
    "            if step%10==0:\n",
    "                dqn_agent.replay()\n",
    "                dqn_agent.target_train()\n",
    "            \n",
    "            cur_state=new_state\n",
    "            \n",
    "            if done:\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if stepno<500:\n",
    "            \n",
    "            if int(env.board.max())==2048:\n",
    "                print(\"Completed in --\",trial)\n",
    "                # print(env.get_board())\n",
    "                max_tile.append(int(env.board.max()))\n",
    "\n",
    "                dqn_agent.save_model(\"success.h5f\")\n",
    "\n",
    "            \n",
    "            else:\n",
    "                # print(f\"Trial number {trial} Failed to complete in 500 steps\")\n",
    "                # print(env.get_board())\n",
    "                print(int(env.board.max()))\n",
    "                max_tile.append(int(env.board.max()))\n",
    "                if int(env.board.max())>=512:\n",
    "                    dqn_agent.save_model(\"trial num-{}.h5f\".format(trial))\n",
    "                    \n",
    "        else:\n",
    "            print(f\"Failed to complete in 500 steps\")\n",
    "            # print(env.get_board())\n",
    "            print(int(env.board.max()))\n",
    "            max_tile.append(int(env.board.max()))\n",
    "            \n",
    "    dqn_agent.save_model(\"final.h5f\")\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f088ad10128>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trialnum=np.arange(100)\n",
    "plt.scatter(trialnum,max_tile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from game2048.game import Game\n",
    "from game2048.displays import Display, IPythonDisplay\n",
    "from game2048.agents import Agent, RandomAgent, ExpectiMaxAgent\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_values(X):\n",
    "    power_mat = np.zeros(shape=(4,4,16),dtype=np.float32)\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if(X[i][j]==0):\n",
    "                power_mat[i][j][0] = 1.0\n",
    "            else:\n",
    "                power = int(math.log(X[i][j],2))\n",
    "                power_mat[i][j][power] = 1.0\n",
    "    return power_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "boards (InputLayer)             (None, 4, 4, 16)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 4, 3, 128)    4224        boards[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 3, 4, 128)    4224        boards[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 2, 128)    32896       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 128)    32896       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 3, 3, 128)    32896       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 2, 4, 128)    32896       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1536)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1024)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1152)         0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1152)         0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 1024)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7424)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1900800     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            1028        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,041,860\n",
      "Trainable params: 2,041,860\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(4,4,16),name='boards')\n",
    "conv1 = layers.Conv2D(filters=128, kernel_size=(1,2),activation='relu')(input_layer)\n",
    "conv2 = layers.Conv2D(filters=128, kernel_size=(2,1),activation='relu')(input_layer)\n",
    "conv3 = layers.Conv2D(filters=128, kernel_size=(1,2),activation='relu')(conv1)\n",
    "conv4 = layers.Conv2D(filters=128, kernel_size=(2,1),activation='relu')(conv1)\n",
    "conv5 = layers.Conv2D(filters=128, kernel_size=(1,2),activation='relu')(conv2)\n",
    "conv6 = layers.Conv2D(filters=128, kernel_size=(2,1),activation='relu')(conv2)\n",
    "conv1 = layers.Flatten()(conv1)\n",
    "conv2 = layers.Flatten()(conv2)\n",
    "conv3 = layers.Flatten()(conv3)\n",
    "conv4 = layers.Flatten()(conv4)\n",
    "conv5 = layers.Flatten()(conv5)\n",
    "conv6 = layers.Flatten()(conv6)\n",
    "concat = layers.concatenate([conv1, conv2, conv3, conv4, conv5, conv6], axis = -1)\n",
    "fullcon = layers.Dense(256)(concat)\n",
    "output = layers.Dense(4, activation='softmax')(fullcon)\n",
    "model = Model(input_layer, output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_boards_actions(BS):\n",
    "    game = Game(4)\n",
    "    agent = ExpectiMaxAgent(game=game)\n",
    "    actions = []\n",
    "    boards = []\n",
    "    while True:\n",
    "        action = agent.step()\n",
    "        board = change_values(game.board)\n",
    "        game.move(action)\n",
    "        action_mat = np.zeros(shape=(4),dtype=np.float32)\n",
    "        action_mat[action] = 1\n",
    "        actions.append(action_mat)\n",
    "        boards.append(board)\n",
    "        if game.end:\n",
    "            game = Game(4)\n",
    "            agent.game = game\n",
    "        if len(actions)==BS:\n",
    "            yield (np.array(boards), np.array(actions))\n",
    "            actions = []\n",
    "            boards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Loaded expectmax lib for 2048: /home/myx/learning_affairs/EE228/project/2048-api/game2048/expectimax/bin/2048.so\n",
      "5000/5000 [==============================] - 30737s 6s/step - loss: 1.1768 - accuracy: 0.4279 - val_loss: 1.1603 - val_accuracy: 0.4533\n",
      "Epoch 2/10\n",
      " 842/5000 [====>.........................] - ETA: 6:43:33 - loss: 1.1435 - accuracy: 0.4493"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-91371562e592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_boards_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TEST\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \tepochs=10)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 500000\n",
    "NUM_TEST = 300\n",
    "BS = 100\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "H = model.fit_generator(\n",
    "\tgenerate_boards_actions(BS),\n",
    "\tsteps_per_epoch=NUM_TRAIN // BS,\n",
    "\tvalidation_data=generate_boards_actions(BS),\n",
    "\tvalidation_steps=NUM_TEST // BS,\n",
    "\tepochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46737993, 0.21000558, 0.19786009, 0.12475433]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('CNN.h5')\n",
    "game = Game(4, random=True)\n",
    "model.predict(change_values(game.board).reshape(1,4,4,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "agent = ExpectiMaxAgent(game=game)\n",
    "print(agent.step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from numpy.random import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from game2048.game import Game\n",
    "from game2048.displays import Display, IPythonDisplay\n",
    "from game2048.agents import Agent, RandomAgent, ExpectiMaxAgent, DCNNAgent\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#フィルタ\n",
    "w_1 = 222\n",
    "w_2 = 222\n",
    "w_3 = 222\n",
    "w_4 = 222\n",
    "w_5 = 222\n",
    "\n",
    "#入力をチャネルごとに分解 (入力 深さ 縦 横 ch)\n",
    "x_image = tf.placeholder(tf.float32, [None, 4, 4, 16])\n",
    "\n",
    "#1層目のチャネルごとの畳み込みフィルタ初期化\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([2,2,16,w_1], stddev=0.1))\n",
    "\n",
    "#1層目の畳み込み処理\n",
    "h_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "#bias\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[w_1]))\n",
    "\n",
    "#1層目のReLU処理\n",
    "relu1 = tf.nn.relu(h_conv1 + b_conv1)\n",
    "\n",
    "#2層目\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([2,2,w_1,w_2], stddev=0.1))\n",
    "h_conv2 = tf.nn.conv2d(relu1, W_conv2, strides=[1,1,1,1], padding='SAME')\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[w_2]))\n",
    "relu2 = tf.nn.relu(h_conv2 + b_conv2)\n",
    "\n",
    "#3層目\n",
    "W_conv3 = tf.Variable(tf.truncated_normal([2,2,w_2,w_3], stddev=0.1))\n",
    "h_conv3 = tf.nn.conv2d(relu2, W_conv3, strides=[1,1,1,1], padding='SAME')\n",
    "b_conv3 = tf.Variable(tf.constant(0.1, shape=[w_3]))\n",
    "relu3 = tf.nn.relu(h_conv3 + b_conv3)\n",
    "\n",
    "#4層目\n",
    "W_conv4 = tf.Variable(tf.truncated_normal([2,2,w_3,w_4], stddev=0.1))\n",
    "h_conv4 = tf.nn.conv2d(relu3, W_conv4, strides=[1,1,1,1], padding='SAME')\n",
    "b_conv4 = tf.Variable(tf.constant(0.1, shape=[w_4]))\n",
    "relu4 = tf.nn.relu(h_conv4 + b_conv4)\n",
    "\n",
    "#5層目\n",
    "W_conv5 = tf.Variable(tf.truncated_normal([2,2,w_4,w_5], stddev=0.1))\n",
    "h_conv5 = tf.nn.conv2d(relu4, W_conv5, strides=[1,1,1,1], padding='SAME')\n",
    "b_conv5 = tf.Variable(tf.constant(0.1, shape=[w_5]))\n",
    "relu5 = tf.nn.relu(h_conv5 + b_conv5)\n",
    "\n",
    "#線形にする\n",
    "relu5_flat = tf.reshape(relu5, [-1, 4*4*w_5])\n",
    "\n",
    "#全結合層/ソフトマックス\n",
    "w0 = tf.Variable(tf.zeros([4*4*w_5, 4]))\n",
    "b0 = tf.Variable(tf.zeros([4]))\n",
    "p = tf.nn.softmax(tf.matmul(relu5_flat, w0) + b0)\n",
    "\n",
    "#学習箇所\n",
    "t = tf.placeholder(tf.float32, [None, 4])\n",
    "loss = -tf.reduce_sum(t * tf.log(tf.clip_by_value(p,1e-10,1.0)))\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "#正答率\n",
    "correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#セッション用意\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/myx/learning_affairs/EE228/project/2048-DCNN/c5_pdata/c5-600000000\n"
     ]
    }
   ],
   "source": [
    "files = '/home/myx/learning_affairs/EE228/project/2048-DCNN/c5_pdata/c5-' + str(600000000)\n",
    "saver.restore(sess, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_values(X):\n",
    "    power_mat = np.zeros(shape=(4,4,16),dtype=np.float32)\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if(X[i][j]==0):\n",
    "                power_mat[i][j][0] = 1.0\n",
    "            else:\n",
    "                power = int(math.log(X[i][j],2))\n",
    "                power_mat[i][j][power] = 1.0\n",
    "    return power_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/myx/learning_affairs/EE228/project/2048-DCNN/c5_pdata/c5-600000000\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable_12 not found in checkpoint\n\t [[Node: save_2/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save_2/RestoreV2', defined at:\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 597, in start\n    self.io_loop.start()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-9faedf753583>\", line 2, in <module>\n    agent = DCNNAgent(game=game)\n  File \"/home/myx/learning_affairs/EE228/project/2048-api/game2048/agents.py\", line 282, in __init__\n    self.saver = tf.train.Saver(max_to_keep=None)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1281, in __init__\n    self.build()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1293, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1330, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 397, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 829, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable_12 not found in checkpoint\n\t [[Node: save_2/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key Variable_12 not found in checkpoint\n\t [[Node: save_2/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1725\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1726\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key Variable_12 not found in checkpoint\n\t [[Node: save_2/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save_2/RestoreV2', defined at:\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 597, in start\n    self.io_loop.start()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-9faedf753583>\", line 2, in <module>\n    agent = DCNNAgent(game=game)\n  File \"/home/myx/learning_affairs/EE228/project/2048-api/game2048/agents.py\", line 282, in __init__\n    self.saver = tf.train.Saver(max_to_keep=None)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1281, in __init__\n    self.build()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1293, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1330, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 397, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 829, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Key Variable_12 not found in checkpoint\n\t [[Node: save_2/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1736\u001b[0m         object_graph_string = reader.get_tensor(\n\u001b[0;32m-> 1737\u001b[0;31m             checkpointable.OBJECT_GRAPH_PROTO_KEY)\n\u001b[0m\u001b[1;32m   1738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    350\u001b[0m         return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),\n\u001b[0;32m--> 351\u001b[0;31m                                           status)\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9faedf753583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCNNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/learning_affairs/EE228/project/2048-api/game2048/agents.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, display)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/myx/learning_affairs/EE228/project/2048-DCNN/c5_pdata/c5-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1743\u001b[0;31m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       \u001b[0;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable_12 not found in checkpoint\n\t [[Node: save_2/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save_2/RestoreV2', defined at:\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 597, in start\n    self.io_loop.start()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-9faedf753583>\", line 2, in <module>\n    agent = DCNNAgent(game=game)\n  File \"/home/myx/learning_affairs/EE228/project/2048-api/game2048/agents.py\", line 282, in __init__\n    self.saver = tf.train.Saver(max_to_keep=None)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1281, in __init__\n    self.build()\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1293, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1330, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 397, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 829, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/myx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable_12 not found in checkpoint\n\t [[Node: save_2/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "game = Game(4, random=False)\n",
    "agent = DCNNAgent(game=game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.play(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(game.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
